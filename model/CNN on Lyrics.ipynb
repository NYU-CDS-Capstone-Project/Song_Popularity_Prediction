{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from collections import Counter\n",
    "import pickle as pkl\n",
    "import random\n",
    "import pdb\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import string\n",
    "\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_id = np.load('id_train.npy')\n",
    "test_id = np.load('id_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lyrics = pd.read_csv(r'final_matrix.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lyrics_train = lyrics[lyrics.ISRC.isin(train_id)]\n",
    "lyrics_test = lyrics[lyrics.ISRC.isin(test_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_emb_matrix():\n",
    "    #load fasttext word vectors\n",
    "    words_to_load = 50000\n",
    "\n",
    "    with open('wiki-news-300d-1M-subword.vec') as f:\n",
    "        #remove the first line\n",
    "        firstLine = f.readline()\n",
    "        loaded_embeddings = np.zeros((words_to_load + 2, 300))\n",
    "        words2id = {}\n",
    "        idx2words = {}\n",
    "        #ordered_words = []\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= words_to_load: \n",
    "                break\n",
    "            s = line.split()\n",
    "            loaded_embeddings[i + 2 , :] = np.asarray(s[1:])\n",
    "            words2id['<pad>'] = PAD_IDX\n",
    "            words2id['<unk>'] = UNK_IDX\n",
    "            words2id[s[0]] = i + 2\n",
    "            idx2words[0] = '<pad>'\n",
    "            idx2words[1] = '<unk>'\n",
    "            idx2words[i + 2] = s[0]\n",
    "   \n",
    "\n",
    "    return words2id,idx2words,loaded_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words2id,idx2words,loaded_embeddings = load_emb_matrix()\n",
    "\n",
    "pkl.dump(words2id, open(f'data/words2id.pkl', 'wb'))\n",
    "pkl.dump(idx2words, open(f'data/idx2words.pkl', 'wb'))\n",
    "pkl.dump(loaded_embeddings, open(f'data/embedding_matrix.pkl', 'wb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "tokenizer = spacy.load('en_core_web_sm')\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# lowercase and remove punctuation\n",
    "def tokenize(sent):\n",
    "    tokens = tokenizer(sent)\n",
    "    return [token.text.lower() for token in tokens \n",
    "            if (token.text not in punctuations) & (token.text not in STOP_WORDS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_dataset(dataset):\n",
    "    token_dataset = []\n",
    "    #all_tokens = []\n",
    "    for sample in dataset:\n",
    "        tokens = tokenize(sample)\n",
    "        token_dataset.append(tokens)\n",
    "        #all_tokens += tokens\n",
    "\n",
    "    return token_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_tokens = tokenize_dataset(lyrics_train['lyrics'])\n",
    "test_tokens = tokenize_dataset(lyrics_test['lyrics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pkl.dump(train_tokens, open(\"data/train_tokens.p\", \"wb\"))\n",
    "pkl.dump(test_tokens, open(\"data/test_tokens.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "# convert token to id in the dataset\n",
    "def token2index_dataset(tokens_data):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [words2id[word] if word in words2id else UNK_IDX for word in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_indices = token2index_dataset(train_tokens)\n",
    "test_data_indices = token2index_dataset(test_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lens = 0.01\n",
    "for i in range(len(train_data_indices)):\n",
    "    lens += len(train_data_indices[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "234"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_SENTENCE_LENGTH = round(lens/len(train_data_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VocabDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_list, target_list, words2id):\n",
    "        \"\"\"\n",
    "        @param data_list: list of character\n",
    "        @param target_list: list of targets\n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list = data_list\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.data_list) == len(self.target_list))\n",
    "        self.words2id = words2id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target_list)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        words_idx = self.data_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[key]\n",
    "        \n",
    "        return [words_idx, len(words_idx),label]\n",
    "\n",
    "def vocab_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all\n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[2])\n",
    "        length_list.append(datum[1])\n",
    "        \n",
    "        \n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec = np.pad(np.array(datum[0]),\n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[1])),\n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list.append(padded_vec)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    #handle torch type problem by adding the following line\n",
    "    data_list = np.asarray(data_list, dtype=int)\n",
    "    label_list = np.array(label_list)\n",
    "    \n",
    "    return [torch.from_numpy(np.array(data_list)), torch.LongTensor(length_list), \n",
    "            torch.LongTensor(label_list)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Stella/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "lyrics_train['label'] = lyrics_train['max_popularity'].apply(lambda x: 1 if x>= 70 else 0 )\n",
    "lyrics_test['label'] = lyrics_test['max_popularity'].apply(lambda x: 1 if x>= 70 else 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_label = list(lyrics_train['label'])\n",
    "test_label = list(lyrics_test['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build train and valid dataloaders\n",
    "\n",
    "train_dataset = VocabDataset(train_data_indices, train_label,words2id)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "\n",
    "test_dataset = VocabDataset(test_data_indices,test_label, words2id)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_weights_matrix(idx2words,loaded_embeddings):\n",
    "   \n",
    "    matrix_len = len(idx2words)\n",
    "    weights_matrix = np.zeros((matrix_len, 300))\n",
    "    \n",
    "    for key in idx2words.keys():\n",
    "        try: \n",
    "            weights_matrix[key] = loaded_embeddings[key]\n",
    "        except KeyError:\n",
    "            weights_matrix[key] = np.random.normal(scale=0.6, size=(emb_dim, ))\n",
    "    return weights_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights_matrix = generate_weights_matrix(idx2words,loaded_embeddings)\n",
    "weights_matrix = torch.from_numpy(weights_matrix).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, weights_matrix, hidden_size, num_layers, num_classes,kernel_size):\n",
    "\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        num_embeddings, embedding_dim = weights_matrix.size()\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim,padding_idx=PAD_IDX)\n",
    "        self.embedding.weight.data.copy_(weights_matrix)\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        \n",
    "        #out: dim = []\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Conv1d(embedding_dim, hidden_size, kernel_size = kernel_size, padding=PAD_IDX),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size = (MAX_SENTENCE_LENGTH - kernel_size + 1),padding = PAD_IDX))\n",
    "        \n",
    "      \n",
    "        \n",
    "        self.fc1 = nn.Linear(hidden_size, 180)\n",
    "        self.fc2 = nn.Linear(180, num_classes)\n",
    "        \n",
    "        self.layer = self.layer.to(device)\n",
    "        self.fc1 = self.fc1.to(device)\n",
    "        self.fc2 = self.fc2.to(device)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "         # Transfer to GPU\n",
    "        # size: [batch_size_x,seq_length_x,hidden_size]\n",
    "        embed = self.embedding(x).to(device)\n",
    "       \n",
    "        #in: dim = [batch_size_x, hidden_size, seq_length_length](after transpose)\n",
    "        #out: dim = []\n",
    "        hidden = self.layer(embed.transpose(1,2))\n",
    "\n",
    "        out = hidden.reshape(hidden.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "        out = F.relu(out.contiguous().view(-1, out.size(-1)))\n",
    "        \n",
    "        logits = self.fc2(out)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loss_val = 0\n",
    "    prediction = []\n",
    "    model.eval()\n",
    "    \n",
    "    for data,lengths,labels in loader:\n",
    "      \n",
    "        labels = labels.to(device)\n",
    "        outputs = F.softmax(model(data), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1] \n",
    "        prediction.append(predicted)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_val += loss.item() * len(data) / len(loader.dataset)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).double().sum().item()\n",
    "    return (100 * correct / total), loss_val,prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/25], Step: [396/396], Training Loss: 0.2534291189726864\n",
      "Epoch: [2/25], Step: [396/396], Training Loss: 0.20358275991658484\n",
      "Epoch: [3/25], Step: [396/396], Training Loss: 0.19107616967957622\n",
      "Epoch: [4/25], Step: [396/396], Training Loss: 0.18054541360516874\n",
      "Epoch: [5/25], Step: [396/396], Training Loss: 0.17253297940073045\n",
      "Epoch: [6/25], Step: [396/396], Training Loss: 0.16183024665557544\n",
      "Epoch: [7/25], Step: [396/396], Training Loss: 0.1543686816463834\n",
      "Epoch: [8/25], Step: [396/396], Training Loss: 0.14290605838497758\n",
      "Epoch: [9/25], Step: [396/396], Training Loss: 0.12658774043812349\n",
      "Epoch: [10/25], Step: [396/396], Training Loss: 0.10606565493157968\n",
      "Epoch: [11/25], Step: [396/396], Training Loss: 0.08284123185666167\n",
      "Epoch: [12/25], Step: [396/396], Training Loss: 0.0681462839290502\n",
      "Epoch: [13/25], Step: [396/396], Training Loss: 0.04482478675012358\n",
      "Epoch: [14/25], Step: [396/396], Training Loss: 0.030633847656094004\n",
      "Epoch: [15/25], Step: [396/396], Training Loss: 0.021056417714037948\n",
      "Epoch: [16/25], Step: [396/396], Training Loss: 0.016634242461496345\n",
      "Epoch: [17/25], Step: [396/396], Training Loss: 0.012493938384133504\n",
      "Epoch: [18/25], Step: [396/396], Training Loss: 0.00846894679524295\n",
      "Epoch: [19/25], Step: [396/396], Training Loss: 0.009404223163452047\n",
      "Epoch: [20/25], Step: [396/396], Training Loss: 0.00924062837624372\n",
      "Epoch: [21/25], Step: [396/396], Training Loss: 0.008080092424339463\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-157-b2a0dc4e69d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-102-c086663ab4f2>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m#in: dim = [batch_size_x, hidden_size, seq_length_length](after transpose)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m#out: dim = []\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         return F.conv1d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 176\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_classes = len(lyrics_train['label'].unique())\n",
    "model = CNN(weights_matrix, hidden_size=300, num_layers=3, num_classes= num_classes, kernel_size= 3)\n",
    "\n",
    "learning_rate = 3e-4\n",
    "num_epochs = 25 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "\n",
    "train_loss_list = []\n",
    "test_loss_list = []\n",
    "accuracy_list = []\n",
    "best_acc = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0\n",
    "    for i, (data_list, lengths, labels) in enumerate(train_loader):\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(data_list)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * len(data_list) / len(train_loader.dataset)\n",
    "    \n",
    "    # validate\n",
    "    test_acc,test_loss,_ = test_model(test_loader, model)\n",
    "    print('Epoch: [{}/{}], Step: [{}/{}], Training Loss: {}'.format(\n",
    "               epoch+1, num_epochs, i+1, len(train_loader), running_loss))\n",
    "\n",
    "    train_loss_list.append(running_loss)\n",
    "    test_loss_list.append(test_loss)\n",
    "    accuracy_list.append(test_acc)\n",
    "    \n",
    "    if test_acc >= best_acc:\n",
    "        best_acc = test_acc\n",
    "    else:\n",
    "        best_acc = best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+iIiMUShLyISQ/4ffdQBbGc/ogUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x14e422e80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(train_loss_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction = test_model(test_loader, model)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction_list = [i.item() for m in prediction for i in m ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88659058487874465"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "f1_score(test_label, prediction_list,average = 'micro') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9144079885877318"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_label.count(0)/len(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.033523537803138374"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_list.count(1)/len(prediction_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.033333333333333333"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(test_label, prediction_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(prediction_list).to_csv('CNN_prediction.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/preetgandhi95/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Requirement already satisfied: en_core_web_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#egg=en_core_web_sm==2.0.0 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (2.0.0)\n",
      "\n",
      "\u001b[93m    Linking successful\u001b[0m\n",
      "    /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages/en_core_web_sm\n",
      "    -->\n",
      "    /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages/spacy/data/en\n",
      "\n",
      "    You can now load the model via spacy.load('en')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run in python console\n",
    "import nltk; nltk.download('stopwords')\n",
    "\n",
    "# Run in terminal or command prompt\n",
    "!python3 -m spacy download en\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyr=pd.read_csv('merged_eng_lyrics_genre_explicit.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repl(e):\n",
    "    return [re.sub('\\n', ' ', sent) for sent in e]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyr=repl(lyr['lyrics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blues=repl(list(lyr[lyr['Blues']==1]['lyrics']))\n",
    "# cg=repl(list(lyr[lyr['Christian/Gospel']==1]['lyrics']))\n",
    "# country=repl(list(lyr[lyr['Country']==1]['lyrics']))\n",
    "# de=repl(list(lyr[lyr['Dance/Electro']==1]['lyrics']))\n",
    "# disco=repl(list(lyr[lyr['Disco']==1]['lyrics']))\n",
    "# dh=repl(list(lyr[lyr['Dutch-House']==1]['lyrics']))\n",
    "# folk=repl(list(lyr[lyr['Folk']==1]['lyrics']))\n",
    "# hiphop=repl(list(lyr[lyr['Hip-Hop']==1]['lyrics']))\n",
    "# indie=repl(list(lyr[lyr['Indie']==1]['lyrics']))\n",
    "# jazz=repl(list(lyr[lyr['Jazz']==1]['lyrics'] ) )\n",
    "# latin=repl(list(lyr[lyr['Latin']==1]['lyrics']))\n",
    "# metal=repl(list(lyr[lyr['Metal']==1]['lyrics']))\n",
    "# other=repl(list(lyr[lyr['Other']==1]['lyrics']))\n",
    "# pop=repl(list(lyr[lyr['Pop']==1]['lyrics']))\n",
    "# rb=repl(list(lyr[lyr['R&B']==1]['lyrics']))\n",
    "# reggae=repl(list(lyr[lyr['Reggae']==1]['lyrics']))\n",
    "# ra=repl(list(lyr[lyr['Rock/Alt']==1]['lyrics']))\n",
    "# rap=repl(list(lyr[lyr['Rock/Alt/Pop']==1]['lyrics']))\n",
    "# missing=repl(list(lyr[lyr['missing']==1]['lyrics']))\n",
    "# explicit=repl(list(lyr[lyr['Explicit']==1]['lyrics']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "lyr_words = list(sent_to_words(lyr))\n",
    "# blues_words = list(sent_to_words(blues))\n",
    "# cg_words = list(sent_to_words(cg))\n",
    "# country_words = list(sent_to_words(country))\n",
    "# de_words = list(sent_to_words(de))\n",
    "# disco_words = list(sent_to_words(disco))\n",
    "# dh_words = list(sent_to_words(dh))\n",
    "# folk_words = list(sent_to_words(folk))\n",
    "# hiphop_words = list(sent_to_words(hiphop))\n",
    "# jazz_words = list(sent_to_words(jazz))\n",
    "# latin_words = list(sent_to_words(latin))\n",
    "# metal_words = list(sent_to_words(metal))\n",
    "# other_words = list(sent_to_words(other))\n",
    "# pop_words = list(sent_to_words(pop))\n",
    "# rb_words = list(sent_to_words(rb))\n",
    "# reggae_words = list(sent_to_words(reggae))\n",
    "# ra_words = list(sent_to_words(ra))\n",
    "# rap_words = list(sent_to_words(rap))\n",
    "# missing_words = list(sent_to_words(missing))\n",
    "# explicit_words = list(sent_to_words(explicit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build(wrd):\n",
    "# Build the bigram and trigram models\n",
    "    bigram = gensim.models.Phrases(wrd, min_count=1, threshold=1) # higher threshold fewer phrases.\n",
    "    trigram = gensim.models.Phrases(bigram[wrd], threshold=1)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "    return bigram_mod,trigram_mod\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    }
   ],
   "source": [
    "lyr_b,lyr_t=build(lyr_words)\n",
    "# blues_b,blues_t=build(blues_words)\n",
    "# cg_b,cg_t=build(cg_words)\n",
    "# country_b,country_t=build(country_words)\n",
    "# de_b,de_t=build(de_words)\n",
    "# disco_b,disco_t=build(disco_words)\n",
    "# dh_b,dh_t=build(dh_words)\n",
    "# folk_b,folk_t=build(folk_words)\n",
    "# hiphop_b,hiphop_t=build(hiphop_words)\n",
    "# jazz_b,jazz_t=build(jazz_words)\n",
    "# latin_b,latin_t=build(latin_words)\n",
    "# metal_b,metal_t=build(metal_words)\n",
    "# other_b,other_t=build(other_words)\n",
    "# pop_b,pop_t=build(pop_words)\n",
    "# rb_b,rb_t=build(rb_words)\n",
    "# reggae_b,reggae_t=build(reggae_words)\n",
    "# ra_b,ra_t=build(ra_words)\n",
    "# rap_b,rap_t=build(rap_words)\n",
    "# missing_b,missing_t=build(missing_words)\n",
    "# explicit_b,explicit_t=build(explicit_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "# def make_bigrams(texts):\n",
    "#     return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "# def make_trigrams(texts):\n",
    "#     return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [lyr_b[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [lyr_t[lyr_b[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lem(data_words):# Remove Stop Words\n",
    "    data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "    # Form Bigrams\n",
    "    data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "    # Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "    # python3 -m spacy download en\n",
    "    nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "    # Do lemmatization keeping only noun, adj, vb, adv\n",
    "    return lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E088] Text of length 1696341 exceeds maximum of 1000000. The v2.x parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-d3095fbfcbab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlyr_lem\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlyr_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# blues_lem=lem(blues_words)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# cg_lem=lem(cg_words)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# country_lem=lem(country_words)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# de_lem=lem(de_words)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-57-4c85e817783c>\u001b[0m in \u001b[0;36mlem\u001b[0;34m(data_words)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Do lemmatization keeping only noun, adj, vb, adv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mlemmatization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_words_bigrams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallowed_postags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'NOUN'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ADJ'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'VERB'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ADV'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-56-7f535560183f>\u001b[0m in \u001b[0;36mlemmatization\u001b[0;34m(texts, allowed_postags)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'parser'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ner'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mtexts_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemma_\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mallowed_postags\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtexts_out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlpclass/lib/python3.6/site-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable)\u001b[0m\n\u001b[1;32m    343\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             raise ValueError(Errors.E088.format(length=len(text),\n\u001b[0;32m--> 345\u001b[0;31m                                                 max_length=self.max_length))\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: [E088] Text of length 1696341 exceeds maximum of 1000000. The v2.x parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`."
     ]
    }
   ],
   "source": [
    "lyr_lem=lem(lyr_words)\n",
    "# blues_lem=lem(blues_words)\n",
    "# cg_lem=lem(cg_words)\n",
    "# country_lem=lem(country_words)\n",
    "# de_lem=lem(de_words)\n",
    "# disco_lem=lem(disco_words)\n",
    "# dh_lem=lem(dh_words)\n",
    "# folk_lem=lem(folk_words)\n",
    "# hiphop_lem=lem(hiphop_words)\n",
    "# jazz_lem=lem(jazz_words)\n",
    "# latin_lem=lem(latin_words)\n",
    "# metal_lem=lem(metal_words)\n",
    "# other_lem=lem(other_words)\n",
    "# pop_lem=lem(pop_words)\n",
    "# rb_lem=lem(rb_words)\n",
    "# reggae_lem =lem(reggae_words)\n",
    "# ra_lem=lem(ra_words)\n",
    "# rap_lem=lem(rap_words)\n",
    "# missing_lem=lem(missing_words)\n",
    "# explicit_lem=lem(explicit_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lemmatized=lyr_lem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=20, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

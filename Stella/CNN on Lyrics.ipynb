{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from collections import Counter\n",
    "import pickle as pkl\n",
    "import random\n",
    "import pdb\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import string\n",
    "\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_id = np.load('id_train.npy')\n",
    "test_id = np.load('id_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics = pd.read_csv(r'final_matrix.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_train = lyrics[lyrics.ISRC.isin(train_id)]\n",
    "lyrics_test = lyrics[lyrics.ISRC.isin(test_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_emb_matrix():\n",
    "    #load fasttext word vectors\n",
    "    words_to_load = 50000\n",
    "\n",
    "    with open('wiki-news-300d-1M-subword.vec') as f:\n",
    "        #remove the first line\n",
    "        firstLine = f.readline()\n",
    "        loaded_embeddings = np.zeros((words_to_load + 2, 300))\n",
    "        words2id = {}\n",
    "        idx2words = {}\n",
    "        #ordered_words = []\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= words_to_load: \n",
    "                break\n",
    "            s = line.split()\n",
    "            loaded_embeddings[i + 2 , :] = np.asarray(s[1:])\n",
    "            words2id['<pad>'] = PAD_IDX\n",
    "            words2id['<unk>'] = UNK_IDX\n",
    "            words2id[s[0]] = i + 2\n",
    "            idx2words[0] = '<pad>'\n",
    "            idx2words[1] = '<unk>'\n",
    "            idx2words[i + 2] = s[0]\n",
    "   \n",
    "\n",
    "    return words2id,idx2words,loaded_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "words2id,idx2words,loaded_embeddings = load_emb_matrix()\n",
    "\n",
    "pkl.dump(words2id, open(f'data/words2id.pkl', 'wb'))\n",
    "pkl.dump(idx2words, open(f'data/idx2words.pkl', 'wb'))\n",
    "pkl.dump(loaded_embeddings, open(f'data/embedding_matrix.pkl', 'wb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "tokenizer = spacy.load('en_core_web_sm')\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# lowercase and remove punctuation\n",
    "def tokenize(sent):\n",
    "    tokens = tokenizer(sent)\n",
    "    return [token.text.lower() for token in tokens \n",
    "            if (token.text not in punctuations) & (token.text not in STOP_WORDS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_dataset(dataset):\n",
    "    token_dataset = []\n",
    "    #all_tokens = []\n",
    "    for sample in dataset:\n",
    "        tokens = tokenize(sample)\n",
    "        token_dataset.append(tokens)\n",
    "        #all_tokens += tokens\n",
    "\n",
    "    return token_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_tokens = tokenize_dataset(lyrics_train['lyrics'])\n",
    "test_tokens = tokenize_dataset(lyrics_test['lyrics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pkl.dump(train_tokens, open(\"data/train_tokens.p\", \"wb\"))\n",
    "pkl.dump(test_tokens, open(\"data/test_tokens.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "# convert token to id in the dataset\n",
    "def token2index_dataset(tokens_data):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [words2id[word] if word in words2id else UNK_IDX for word in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_indices = token2index_dataset(train_tokens)\n",
    "test_data_indices = token2index_dataset(test_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = 0.01\n",
    "for i in range(len(train_data_indices)):\n",
    "    lens += len(train_data_indices[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "234"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_SENTENCE_LENGTH = round(lens/len(train_data_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VocabDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_list, target_list, words2id):\n",
    "        \"\"\"\n",
    "        @param data_list: list of character\n",
    "        @param target_list: list of targets\n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list = data_list\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.data_list) == len(self.target_list))\n",
    "        self.words2id = words2id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target_list)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        words_idx = self.data_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[key]\n",
    "        \n",
    "        return [words_idx, len(words_idx),label]\n",
    "\n",
    "def vocab_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all\n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[2])\n",
    "        length_list.append(datum[1])\n",
    "        \n",
    "        \n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec = np.pad(np.array(datum[0]),\n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[1])),\n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list.append(padded_vec)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    #handle torch type problem by adding the following line\n",
    "    data_list = np.asarray(data_list, dtype=int)\n",
    "    label_list = np.array(label_list)\n",
    "    \n",
    "    return [torch.from_numpy(np.array(data_list)), torch.LongTensor(length_list), \n",
    "            torch.LongTensor(label_list)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Stella/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "lyrics_train['label'] = lyrics_train['max_popularity'].apply(lambda x: 1 if x>= 70 else 0 )\n",
    "lyrics_test['label'] = lyrics_test['max_popularity'].apply(lambda x: 1 if x>= 70 else 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_label = list(lyrics_train['label'])\n",
    "test_label = list(lyrics_test['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build train and valid dataloaders\n",
    "\n",
    "train_dataset = VocabDataset(train_data_indices, train_label,words2id)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "\n",
    "test_dataset = VocabDataset(test_data_indices,test_label, words2id)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_weights_matrix(idx2words,loaded_embeddings):\n",
    "   \n",
    "    matrix_len = len(idx2words)\n",
    "    weights_matrix = np.zeros((matrix_len, 300))\n",
    "    \n",
    "    for key in idx2words.keys():\n",
    "        try: \n",
    "            weights_matrix[key] = loaded_embeddings[key]\n",
    "        except KeyError:\n",
    "            weights_matrix[key] = np.random.normal(scale=0.6, size=(emb_dim, ))\n",
    "    return weights_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_matrix = generate_weights_matrix(idx2words,loaded_embeddings)\n",
    "weights_matrix = torch.from_numpy(weights_matrix).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, weights_matrix, hidden_size, num_layers, num_classes,kernel_size):\n",
    "\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        num_embeddings, embedding_dim = weights_matrix.size()\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim,padding_idx=PAD_IDX)\n",
    "        self.embedding.weight.data.copy_(weights_matrix)\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        \n",
    "        #out: dim = []\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Conv1d(embedding_dim, hidden_size, kernel_size = kernel_size, padding=PAD_IDX),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size = (MAX_SENTENCE_LENGTH - kernel_size + 1),padding = PAD_IDX))\n",
    "        \n",
    "      \n",
    "        \n",
    "        self.fc1 = nn.Linear(hidden_size, 180)\n",
    "        self.fc2 = nn.Linear(180, num_classes)\n",
    "        \n",
    "        self.layer = self.layer.to(device)\n",
    "        self.fc1 = self.fc1.to(device)\n",
    "        self.fc2 = self.fc2.to(device)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "         # Transfer to GPU\n",
    "        # size: [batch_size_x,seq_length_x,hidden_size]\n",
    "        embed = self.embedding(x).to(device)\n",
    "       \n",
    "        #in: dim = [batch_size_x, hidden_size, seq_length_length](after transpose)\n",
    "        #out: dim = []\n",
    "        hidden = self.layer(embed.transpose(1,2))\n",
    "\n",
    "        out = hidden.reshape(hidden.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "        out = F.relu(out.contiguous().view(-1, out.size(-1)))\n",
    "        \n",
    "        logits = self.fc2(out)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loss_val = 0\n",
    "    model.eval()\n",
    "    \n",
    "    for data,lengths,labels in loader:\n",
    "      \n",
    "        labels = labels.to(device)\n",
    "        outputs = F.softmax(model(data), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1] \n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_val += loss.item() * len(data) / len(loader.dataset)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).double().sum().item()\n",
    "    return (100 * correct / total), loss_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10], Step: [396/396], Training Loss: 0.26503872755457847\n",
      "Epoch: [2/10], Step: [396/396], Training Loss: 0.20568980406584494\n",
      "Epoch: [3/10], Step: [396/396], Training Loss: 0.19262487070125267\n",
      "Epoch: [4/10], Step: [396/396], Training Loss: 0.18318751624235508\n",
      "Epoch: [5/10], Step: [396/396], Training Loss: 0.1725580351097492\n",
      "Epoch: [6/10], Step: [396/396], Training Loss: 0.16377817334914027\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(lyrics_train['label'].unique())\n",
    "model = CNN(weights_matrix, hidden_size=200, num_layers=2, num_classes= num_classes, kernel_size= 3)\n",
    "\n",
    "learning_rate = 3e-4\n",
    "num_epochs = 10 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "\n",
    "train_loss_list = []\n",
    "test_loss_list = []\n",
    "accuracy_list = []\n",
    "best_acc = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0\n",
    "    for i, (data_list, lengths, labels) in enumerate(train_loader):\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(data_list)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * len(data_list) / len(train_loader.dataset)\n",
    "    \n",
    "    # validate\n",
    "    test_acc,test_loss = test_model(test_loader, model)\n",
    "    print('Epoch: [{}/{}], Step: [{}/{}], Training Loss: {}'.format(\n",
    "               epoch+1, num_epochs, i+1, len(train_loader), running_loss))\n",
    "\n",
    "    train_loss_list.append(running_loss)\n",
    "    test_loss_list.append(test_loss)\n",
    "    accuracy_list.append(test_acc)\n",
    "    \n",
    "    if test_acc >= best_acc:\n",
    "        best_acc = test_acc\n",
    "    else:\n",
    "        best_acc = best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
